{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4594c11c",
   "metadata": {},
   "source": [
    "Modeling\n",
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd688ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970606f",
   "metadata": {},
   "source": [
    "Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "125298f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cc3109",
   "metadata": {},
   "source": [
    "Trying to do predictions only based on text. Maybe later bring look at keywords, location, and id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76d71a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.drop(columns=['keyword', 'location','id'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da816dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3537ffc2",
   "metadata": {},
   "source": [
    "Function for tokenizing and lowercasing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6398eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(text):\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "    y = []\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            y.append(i)\n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "            y.append(i)\n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    for i in text:\n",
    "        y.append(ps.stem(i))\n",
    "    return \" \".join(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e14e2",
   "metadata": {},
   "source": [
    "Main data set I am working on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c71fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['transform_text'] = tr['text'].apply(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7269a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>transform_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask place notifi offic evacu shelter pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>two giant crane hold bridg collaps nearbi home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>thetawniest control wild fire california even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>utc 5km volcano hawaii http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>polic investig collid car littl portug rider s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>latest home raze northern california wildfir a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                         transform_text  \n",
       "0             deed reason earthquak may allah forgiv us  \n",
       "1                  forest fire near la rong sask canada  \n",
       "2     resid ask place notifi offic evacu shelter pla...  \n",
       "3           peopl receiv wildfir evacu order california  \n",
       "4     got sent photo rubi alaska smoke wildfir pour ...  \n",
       "...                                                 ...  \n",
       "7608  two giant crane hold bridg collaps nearbi home...  \n",
       "7609  thetawniest control wild fire california even ...  \n",
       "7610                        utc 5km volcano hawaii http  \n",
       "7611  polic investig collid car littl portug rider s...  \n",
       "7612  latest home raze northern california wildfir a...  \n",
       "\n",
       "[7613 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1925edc",
   "metadata": {},
   "source": [
    "Dropping the original text for the altered text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a100c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.drop(columns=['text'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85bf2a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>target</th>\n",
       "      <th>transform_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask place notifi offic evacu shelter pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>two giant crane hold bridg collaps nearbi home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>thetawniest control wild fire california even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>utc 5km volcano hawaii http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>polic investig collid car littl portug rider s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>latest home raze northern california wildfir a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  target  \\\n",
       "0         1     NaN      NaN       1   \n",
       "1         4     NaN      NaN       1   \n",
       "2         5     NaN      NaN       1   \n",
       "3         6     NaN      NaN       1   \n",
       "4         7     NaN      NaN       1   \n",
       "...     ...     ...      ...     ...   \n",
       "7608  10869     NaN      NaN       1   \n",
       "7609  10870     NaN      NaN       1   \n",
       "7610  10871     NaN      NaN       1   \n",
       "7611  10872     NaN      NaN       1   \n",
       "7612  10873     NaN      NaN       1   \n",
       "\n",
       "                                         transform_text  \n",
       "0             deed reason earthquak may allah forgiv us  \n",
       "1                  forest fire near la rong sask canada  \n",
       "2     resid ask place notifi offic evacu shelter pla...  \n",
       "3           peopl receiv wildfir evacu order california  \n",
       "4     got sent photo rubi alaska smoke wildfir pour ...  \n",
       "...                                                 ...  \n",
       "7608  two giant crane hold bridg collaps nearbi home...  \n",
       "7609  thetawniest control wild fire california even ...  \n",
       "7610                        utc 5km volcano hawaii http  \n",
       "7611  polic investig collid car littl portug rider s...  \n",
       "7612  latest home raze northern california wildfir a...  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872192b",
   "metadata": {},
   "source": [
    "Setting up for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e2f4ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tr[\"transform_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a33c76ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tr[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75408d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de00d7",
   "metadata": {},
   "source": [
    "Vectorizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "613c3540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "cv = CountVectorizer()\n",
    "tf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0b2f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec = tf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bc7c0f",
   "metadata": {},
   "source": [
    "Setting up the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da994571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "621e208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import regex as re\n",
    "import string\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, cross_val_score, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e931b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_tf = Pipeline([('Vectorizer',  TfidfVectorizer(lowercase=False)),\n",
    "               ('mnb', MultinomialNB())])\n",
    "\n",
    "lr_tf = Pipeline([('Vectorizer', TfidfVectorizer(lowercase=False)),\n",
    "               ('LogisticReg', LogisticRegression(max_iter=200, random_state=1))])\n",
    "\n",
    "dtc_tf = Pipeline([('Vectorizer', TfidfVectorizer(lowercase=False)),\n",
    "               ('DecisionTree', DecisionTreeClassifier(random_state=1))])\n",
    "\n",
    "rf_tf = Pipeline([('Vectorizer', TfidfVectorizer(lowercase=False)),\n",
    "               ('RandomFor', RandomForestClassifier(random_state=1))]) \n",
    "\n",
    "gbc_tf = Pipeline([('Vectorizer', TfidfVectorizer(lowercase=False)),\n",
    "               ('gradiendboosting', GradientBoostingClassifier(random_state=1))])\n",
    "\n",
    "svc_tf = Pipeline([('Vectorizer', TfidfVectorizer(lowercase=False)),\n",
    "                ('SupportVec', SVC(random_state=1))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48603fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models2 = [('MultiNomBa', mnb_tf),\n",
    "          ('LogisticReg', lr_tf),\n",
    "          ('DecTreeClass', dtc_tf),           \n",
    "          ('RandomFor', rf_tf),\n",
    "          ('GradBoost', gbc_tf),\n",
    "          ('SupportVec', svc_tf)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3d74dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mba=0\n",
    "num_lreg=1\n",
    "num_dtc=2\n",
    "num_rfc=3\n",
    "num_gbc=4\n",
    "num_svc=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "089de354",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e655c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gridsearch_tf(params, name, models, num):\n",
    "    for model, grid in params.items():\n",
    "        print(model, 'Grid Search:')\n",
    "        print(model)\n",
    "        pipe = Pipeline(steps=[('Vectorizer', TfidfVectorizer(lowercase=False)),\n",
    "                                ('classifier', models[num][1][1])]) \n",
    "        print(pipe[\"Vectorizer\"])\n",
    "        gridsearch = GridSearchCV(estimator=pipe, param_grid=grid[0], scoring='accuracy', cv=5)\n",
    "        gridsearch.fit(X, y)\n",
    "        print(\"Scoring method: Recall\")\n",
    "        print(f'Avg of cross validation scores: {gridsearch.cv_results_[\"mean_test_score\"]}')\n",
    "        print(f'Best cross validation score: {gridsearch.best_score_ :.2%}')\n",
    "        print(f'Optimal parameters: {gridsearch.best_params_}')\n",
    "        tuned_params[name] = gridsearch.best_params_, gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b49f1b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_params1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bfe7c91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression score: <bound method BaseSearchCV.score of GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('vect',\n",
      "                                        TfidfVectorizer(stop_words='english')),\n",
      "                                       ('clf', LogisticRegression())]),\n",
      "             param_grid={'clf__C': array([1.00000000e-05, 3.16227766e-04, 1.00000000e-02, 3.16227766e-01,\n",
      "       1.00000000e+01])},\n",
      "             scoring='accuracy')>\n",
      "Random Forest score: <bound method BaseSearchCV.score of GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('vect',\n",
      "                                        TfidfVectorizer(stop_words='english')),\n",
      "                                       ('clf', RandomForestClassifier())]),\n",
      "             param_grid={'clf__n_estimators': (100, 200)}, scoring='accuracy')>\n"
     ]
    }
   ],
   "source": [
    "names = [\n",
    "         \"Logistic Regression\",\n",
    "         \"Random Forest\",\n",
    "        ]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    RandomForestClassifier(),\n",
    "]\n",
    "\n",
    "parameters = [\n",
    "              {'clf__C': (np.logspace(-5, 1, 5))},\n",
    "    \n",
    "              {\n",
    "               'clf__n_estimators': (100, 200)                  \n",
    "               }\n",
    "             ]\n",
    "\n",
    "for name, classifier, params in zip(names, classifiers, parameters):\n",
    "    clf_pipe = Pipeline([\n",
    "        ('vect', TfidfVectorizer(stop_words='english')),\n",
    "        ('clf', classifier),\n",
    "    ])\n",
    "    gs_clf = GridSearchCV(clf_pipe, param_grid=params, scoring='accuracy', cv=5)\n",
    "    clf = gs_clf.fit(X, y)\n",
    "    score = clf.score\n",
    "    tuned_params1[name] = clf.best_params_, clf.best_score_\n",
    "    print(\"{} score: {}\".format(name, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e227e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': ({'clf__C': 0.31622776601683794}, 0.6936903528291126),\n",
       " 'Random Forest': ({'clf__n_estimators': 200}, 0.6494215286759395)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_params1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b336be4",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3f2c496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticReg Grid Search:\n",
      "LogisticReg\n",
      "TfidfVectorizer(lowercase=False)\n",
      "Scoring method: Accuracy\n",
      "Avg of cross validation scores: [       nan 0.4        0.         0.                nan        nan\n",
      "        nan 0.4        0.         0.                nan        nan\n",
      "        nan 0.         0.67779303 0.67779303        nan        nan\n",
      "        nan 0.         0.67779303 0.67779303        nan        nan\n",
      "        nan 0.20023017 0.29044051 0.29074632        nan        nan\n",
      "        nan 0.20023017 0.29044051 0.29074632        nan        nan\n",
      "        nan 0.22762378 0.64905946 0.64905946        nan        nan\n",
      "        nan 0.22762378 0.64905946 0.64905946        nan        nan\n",
      "        nan 0.44085954 0.55948736 0.56009898        nan        nan\n",
      "        nan 0.44085954 0.55948736 0.56009898        nan        nan\n",
      "        nan 0.54632257 0.65059551 0.65059551        nan        nan\n",
      "        nan 0.54632257 0.65059551 0.65059551        nan        nan]\n",
      "Best cross validation score: 67.78%\n",
      "Optimal parameters: {'classifier__C': 0.001, 'classifier__fit_intercept': False, 'classifier__max_iter': 100, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "params_lr_cv1 = {'LogisticReg': [{\n",
    "    \"classifier__penalty\":[\"l1\", \"l2\", \"elasticnet\"],\n",
    "    'classifier__max_iter':[100, 200],\n",
    "    'classifier__C':[0.001, 0.1, 1],\n",
    "    'classifier__solver':['lbfgs', 'saga'],\n",
    "    'classifier__fit_intercept':[True, False]\n",
    "\n",
    "}]}\n",
    "\n",
    "gridsearch_tf(params_lr_cv1, name=\"LogisticReg\", models=models2, num=num_lreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0587a378",
   "metadata": {},
   "source": [
    "Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "244a7e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree Grid Search:\n",
      "DecisionTree\n",
      "TfidfVectorizer(lowercase=False)\n",
      "Scoring method: Accuracy\n",
      "Avg of cross validation scores: [0.6194726  0.6194726  0.6194726  0.6194726  0.62459502 0.62459502\n",
      " 0.62459502 0.62459502 0.61290411 0.61290411 0.61290411 0.61290411\n",
      " 0.60764614 0.60764614 0.6093533  0.60869661 0.59858525 0.59727231\n",
      " 0.5984541  0.59806023 0.61080109 0.60948772 0.6113262  0.61263974\n",
      " 0.6194726  0.6194726  0.6194726  0.6194726  0.6244637  0.6244637\n",
      " 0.6244637  0.6244637  0.61382369 0.61382369 0.61382369 0.61382369\n",
      " 0.60357531 0.60357531 0.60357531 0.59950199 0.59648379 0.59819146\n",
      " 0.5970095  0.59595885 0.61277011 0.61329582 0.6111941  0.61171964\n",
      " 0.6194726  0.6194726  0.6194726  0.6194726  0.6194726  0.6194726\n",
      " 0.6194726  0.6194726  0.6194726  0.6194726  0.6194726  0.6194726\n",
      " 0.6194726  0.6194726  0.6194726  0.6194726  0.6194726  0.6194726\n",
      " 0.6194726  0.6194726  0.6194726  0.6194726  0.6194726  0.6194726\n",
      " 0.6194726  0.6194726  0.6194726  0.6194726  0.62012937 0.62012937\n",
      " 0.62012937 0.62012937 0.62012937 0.62012937 0.62012937 0.62012937\n",
      " 0.62012937 0.62012937 0.62012937 0.62012937 0.62012937 0.62012937\n",
      " 0.62012937 0.62012937 0.62012937 0.62012937 0.62012937 0.62012937\n",
      " 0.5703402  0.5703402  0.5703402  0.5703402  0.5703402  0.5703402\n",
      " 0.5703402  0.5703402  0.5703402  0.5703402  0.5703402  0.5703402\n",
      " 0.5703402  0.5703402  0.5703402  0.5703402  0.5703402  0.5703402\n",
      " 0.5703402  0.5703402  0.5703402  0.5703402  0.5703402  0.5703402\n",
      " 0.5703402  0.5703402  0.5703402  0.5703402  0.5703402  0.5703402\n",
      " 0.5703402  0.5703402  0.5703402  0.5703402  0.5703402  0.5703402\n",
      " 0.5703402  0.5703402  0.5703402  0.5703402  0.5703402  0.5703402\n",
      " 0.5703402  0.5703402  0.5703402  0.5703402  0.5703402  0.5703402 ]\n",
      "Best cross validation score: 62.46%\n",
      "Optimal parameters: {'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'gini', 'classifier__max_depth': 3, 'classifier__min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "params_dtc1 = {'DecisionTree': [{\n",
    "    'classifier__criterion':['gini', 'entropy'],\n",
    "    'classifier__max_depth':[1, 3, 5, 10, 15, 25],\n",
    "    'classifier__min_samples_split':[2, 5, 6, 8],\n",
    "    'classifier__ccp_alpha':[0.0, 0.01, 0.1]\n",
    "}]}\n",
    "\n",
    "gridsearch_tf(params_dtc1, name='DecisionTree', models=models2, num=num_dtc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6d456",
   "metadata": {},
   "source": [
    "Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d5ebd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Grid Search:\n",
      "RandomForest\n",
      "TfidfVectorizer(lowercase=False)\n",
      "Scoring method: Recall\n",
      "Avg of cross validation scores: [0.48090436 0.49374746 0.49741765 0.50261643 0.50353386 0.48121017\n",
      " 0.49436002 0.49894437 0.49619348 0.49435955]\n",
      "Best cross validation score: 50.35%\n",
      "Optimal parameters: {'classifier__criterion': 'gini', 'classifier__n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "params_rf1 = {'RandomForest': [{\n",
    "    \"classifier__n_estimators\": [50,100, 150, 200, 250],\n",
    "    'classifier__criterion':['gini', 'entropy'],\n",
    "#    \"classifier__max_depth\": [2, 3, 4, 5, 8, 12, 20],\n",
    "#    \"classifier__min_samples_leaf\": [2, 4, 6]\n",
    "#    'classifier__max_depth':[5, 10, 15, 20],\n",
    "#    \"classifier__min_weight_fraction_leaf\": [0.1, 0.3]\n",
    "}]}\n",
    "\n",
    "gridsearch_tf(params_rf1, name='RandomForest', models=models2, num=num_rfc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f6812bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Grid Search:\n",
      "RandomForest\n",
      "TfidfVectorizer(lowercase=False)\n",
      "Scoring method: Recall\n",
      "Avg of cross validation scores: [0.65204758 0.65125785 0.65388623 0.65559494 0.65559459 0.65651452\n",
      " 0.65664662 0.65809053 0.65822159 0.65730227]\n",
      "Best cross validation score: 65.82%\n",
      "Optimal parameters: {'classifier__criterion': 'entropy', 'classifier__n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "params_rf1 = {'RandomForest': [{\n",
    "    \"classifier__n_estimators\": [50,100, 150, 200, 250],\n",
    "    'classifier__criterion':['gini', 'entropy'],\n",
    "#    \"classifier__max_depth\": [2, 3, 4, 5, 8, 12, 20],\n",
    "#    \"classifier__min_samples_leaf\": [2, 4, 6]\n",
    "#    'classifier__max_depth':[5, 10, 15, 20],\n",
    "#    \"classifier__min_weight_fraction_leaf\": [0.1, 0.3]\n",
    "}]}\n",
    "\n",
    "gridsearch_tf(params_rf1, name='RandomForest', models=models2, num=num_rfc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f01c30",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c87fc",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d16d3870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Grid Search:\n",
      "MultinomialNB\n",
      "TfidfVectorizer(lowercase=False)\n",
      "Scoring method: Accuracy\n",
      "Avg of cross validation scores: [0.67450826 0.67897477 0.68501816 0.69316335 0.70183304 0.70590508\n",
      " 0.7118161  0.71221058 0.71431247]\n",
      "Best cross validation score: 71.43%\n",
      "Optimal parameters: {'classifier__alpha': 1}\n"
     ]
    }
   ],
   "source": [
    "params_nb_cv1 = {'MultinomialNB': [{\n",
    "    'classifier__alpha':[.001, .01, .05, .1, .2, .4, .6, .8, 1],\n",
    "\n",
    "}]}\n",
    "\n",
    "gridsearch_tf(params_nb_cv1, name=\"MultinomialNB\", models=models2, num=num_mba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761edc2f",
   "metadata": {},
   "source": [
    "Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ea3e8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradBoostClassifier Grid Search:\n",
      "GradBoostClassifier\n",
      "CountVectorizer(lowercase=False)\n",
      "Scoring method: Accuracy\n",
      "Avg of cross validation scores: [0.5703402  0.59542719 0.5703402  0.59333091 0.62603902 0.63943795\n",
      " 0.62380641 0.62525274]\n",
      "Best cross validation score: 63.94%\n",
      "Optimal parameters: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 5, 'classifier__n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "params_gbc1 = {'GradBoostClassifier': [{\n",
    "    'classifier__learning_rate':[.001, .01],\n",
    "    'classifier__n_estimators':[100, 200],\n",
    "    'classifier__max_depth':[5, 10]\n",
    "}]}\n",
    "\n",
    "gridsearch_count(params_gbc1, name='GradBoostClassifier1', models=models2, num=num_gbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceffc07f",
   "metadata": {},
   "source": [
    "Support Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea6d1429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Grid Search:\n",
      "SVC\n",
      "CountVectorizer(lowercase=False)\n",
      "Scoring method: Accuracy\n",
      "Avg of cross validation scores: [0.65244231]\n",
      "Best cross validation score: 65.24%\n",
      "Optimal parameters: {'classifier__C': 1, 'classifier__gamma': 'scale', 'classifier__kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "params_svc1 = {'SVC': [{\n",
    "    'classifier__C':[1],\n",
    "    'classifier__kernel':['linear'],\n",
    "    'classifier__gamma':['scale'],\n",
    "}]}\n",
    "\n",
    "gridsearch_count(params_svc1, name='SVC1', models=models2, num=num_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01b1ad0",
   "metadata": {},
   "source": [
    "These grid searches were significantly lower than the defaults run in the exploratory models notebook so there is something wrong with these grid searches. Random forest is because the max depth was probably significantly higher but the others don't make sense. It is possible this is because there was no shuffle split. It is possible the data would be better if shuffled. This can be done in grid search by train test split."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
