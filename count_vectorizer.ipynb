{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4594c11c",
   "metadata": {},
   "source": [
    "Grid search attempt using count vectorized text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd688ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970606f",
   "metadata": {},
   "source": [
    "Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "125298f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d6d03c",
   "metadata": {},
   "source": [
    "Trying to do predictions only based on text. Maybe later bring look at keywords, location, and id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d71a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['keyword', 'location','id'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da816dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3537ffc2",
   "metadata": {},
   "source": [
    "Function for tokenizing and lowercasing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c6398eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(text):\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "    y = []\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            y.append(i)\n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "            y.append(i)\n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    for i in text:\n",
    "        y.append(ps.stem(i))\n",
    "    return \" \".join(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e14e2",
   "metadata": {},
   "source": [
    "Main data set I am working on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c71fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['transform_text'] = tr['text'].apply(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7269a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>transform_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask place notifi offic evacu shelter pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>two giant crane hold bridg collaps nearbi home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>thetawniest control wild fire california even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>utc 5km volcano hawaii http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>polic investig collid car littl portug rider s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>latest home raze northern california wildfir a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                         transform_text  \n",
       "0             deed reason earthquak may allah forgiv us  \n",
       "1                  forest fire near la rong sask canada  \n",
       "2     resid ask place notifi offic evacu shelter pla...  \n",
       "3           peopl receiv wildfir evacu order california  \n",
       "4     got sent photo rubi alaska smoke wildfir pour ...  \n",
       "...                                                 ...  \n",
       "7608  two giant crane hold bridg collaps nearbi home...  \n",
       "7609  thetawniest control wild fire california even ...  \n",
       "7610                        utc 5km volcano hawaii http  \n",
       "7611  polic investig collid car littl portug rider s...  \n",
       "7612  latest home raze northern california wildfir a...  \n",
       "\n",
       "[7613 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1925edc",
   "metadata": {},
   "source": [
    "Dropping the original text for the altered text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a100c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.drop(columns=['text'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85bf2a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>transform_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>resid ask place notifi offic evacu shelter pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>1</td>\n",
       "      <td>two giant crane hold bridg collaps nearbi home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>1</td>\n",
       "      <td>thetawniest control wild fire california even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>1</td>\n",
       "      <td>utc 5km volcano hawaii http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>1</td>\n",
       "      <td>polic investig collid car littl portug rider s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>1</td>\n",
       "      <td>latest home raze northern california wildfir a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                     transform_text\n",
       "0          1          deed reason earthquak may allah forgiv us\n",
       "1          1               forest fire near la rong sask canada\n",
       "2          1  resid ask place notifi offic evacu shelter pla...\n",
       "3          1        peopl receiv wildfir evacu order california\n",
       "4          1  got sent photo rubi alaska smoke wildfir pour ...\n",
       "...      ...                                                ...\n",
       "7608       1  two giant crane hold bridg collaps nearbi home...\n",
       "7609       1  thetawniest control wild fire california even ...\n",
       "7610       1                        utc 5km volcano hawaii http\n",
       "7611       1  polic investig collid car littl portug rider s...\n",
       "7612       1  latest home raze northern california wildfir a...\n",
       "\n",
       "[7613 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872192b",
   "metadata": {},
   "source": [
    "Setting up for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e2f4ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tr[\"transform_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a33c76ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tr[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d53b585",
   "metadata": {},
   "source": [
    "It makes more sense to work on averaged cross vals than the train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75408d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de00d7",
   "metadata": {},
   "source": [
    "Vectorizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "613c3540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "cv = CountVectorizer()\n",
    "tf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da994571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "621e208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import regex as re\n",
    "import string\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, cross_val_score, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c18de40",
   "metadata": {},
   "source": [
    "Pipeline being set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e931b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_tf = Pipeline([('Vectorizer',  TfidfVectorizer(lowercase=False)),\n",
    "               ('mnb', MultinomialNB())])\n",
    "\n",
    "lr_tf = Pipeline([('Vectorizer', TfidfVectorizer(lowercase=False)),\n",
    "               ('LogisticReg', LogisticRegression(max_iter=200, random_state=1))])\n",
    "\n",
    "dtc_tf = Pipeline([('Vectorizer', TfidfVectorizer(lowercase=False)),\n",
    "               ('DecisionTree', DecisionTreeClassifier(random_state=1))])\n",
    "\n",
    "rf_tf = Pipeline([('Vectorizer', TfidfVectorizer(lowercase=False)),\n",
    "               ('RandomFor', RandomForestClassifier(random_state=1))]) \n",
    "\n",
    "gbc_tf = Pipeline([('Vectorizer', TfidfVectorizer(lowercase=False)),\n",
    "               ('gradiendboosting', GradientBoostingClassifier(random_state=1))])\n",
    "\n",
    "svc_tf = Pipeline([('Vectorizer', TfidfVectorizer(lowercase=False)),\n",
    "                ('SupportVec', SVC(random_state=1))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "48603fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models2 = [('MultiNomBa', mnb_tf),\n",
    "          ('LogisticReg', lr_tf),\n",
    "          ('DecTreeClass', dtc_tf),           \n",
    "          ('RandomFor', rf_tf),\n",
    "          ('GradBoost', gbc_tf),\n",
    "          ('SupportVec', svc_tf)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3d74dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mba=0\n",
    "num_lreg=1\n",
    "num_dtc=2\n",
    "num_rfc=3\n",
    "num_gbc=4\n",
    "num_svc=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c39f31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e655c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gridsearch_count(params, name, models, num):\n",
    "    for model, grid in params.items():\n",
    "        print(model, 'Grid Search:')\n",
    "        print(model)\n",
    "        pipe = Pipeline(steps=[('Vectorizer', CountVectorizer(lowercase=False)),\n",
    "                                ('classifier', models[num][1][1])]) \n",
    "        print(pipe[\"Vectorizer\"])\n",
    "        gridsearch = GridSearchCV(estimator=pipe, param_grid=grid[0], scoring='accuracy', cv=5)\n",
    "        gridsearch.fit(X, y)\n",
    "        print(\"Scoring method: Accuracy\")\n",
    "        print(f'Avg of cross validation scores: {gridsearch.cv_results_[\"mean_test_score\"]}')\n",
    "        print(f'Best cross validation score: {gridsearch.best_score_ :.2%}')\n",
    "        print(f'Optimal parameters: {gridsearch.best_params_}')\n",
    "        tuned_params[name] = gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cafaa2e",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3f2c496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticReg Grid Search:\n",
      "LogisticReg\n",
      "CountVectorizer(lowercase=False)\n",
      "Scoring method: Accuracy\n",
      "Avg of cross validation scores: [       nan 0.5703402  0.57809341 0.57730515        nan        nan\n",
      "        nan 0.5703402  0.57809341 0.57783069        nan        nan\n",
      "        nan 0.5703402  0.63090156 0.63090156        nan        nan\n",
      "        nan 0.5703402  0.63090156 0.63090156        nan        nan\n",
      "        nan 0.6458765  0.70826935 0.70708635        nan        nan\n",
      "        nan 0.64548245 0.70826935 0.70866279        nan        nan\n",
      "        nan 0.65572807 0.67963482 0.67963482        nan        nan\n",
      "        nan 0.65572807 0.67963482 0.67963482        nan        nan\n",
      "        nan 0.6588775  0.68751582 0.68685922        nan        nan\n",
      "        nan 0.65388588 0.68751582 0.68607105        nan        nan\n",
      "        nan 0.63457955 0.64876622 0.65178813        nan        nan\n",
      "        nan 0.63392174 0.64876622 0.64902912        nan        nan]\n",
      "Best cross validation score: 70.87%\n",
      "Optimal parameters: {'classifier__C': 0.1, 'classifier__fit_intercept': True, 'classifier__max_iter': 200, 'classifier__penalty': 'l2', 'classifier__solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "params_lr_cv1 = {'LogisticReg': [{\n",
    "    \"classifier__penalty\":[\"l1\", \"l2\", \"elasticnet\"],\n",
    "    'classifier__max_iter':[100, 200],\n",
    "    'classifier__C':[0.001, 0.1, 1],\n",
    "    'classifier__solver':['lbfgs', 'saga'],\n",
    "    'classifier__fit_intercept':[True, False]\n",
    "\n",
    "}]}\n",
    "\n",
    "run_gridsearch_count(params_lr_cv1, name=\"LogisticReg\", models=models2, num=num_lreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0141e50e",
   "metadata": {},
   "source": [
    "Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "244a7e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree Grid Search:\n",
      "DecisionTree\n",
      "CountVectorizer(lowercase=False)\n",
      "Scoring method: Accuracy\n",
      "Avg of cross validation scores: [0.618816   0.618816   0.618816   0.618816   0.62354437 0.62354437\n",
      " 0.62354437 0.62354437 0.61185355 0.61185355 0.61185355 0.61185355\n",
      " 0.60252579 0.60344512 0.6033138  0.60305107 0.59963684 0.59819224\n",
      " 0.59884884 0.5993742  0.61369185 0.6131664  0.61447977 0.61277236\n",
      " 0.618816   0.618816   0.618816   0.618816   0.62354437 0.62354437\n",
      " 0.62354437 0.62354437 0.61027771 0.61027771 0.61027771 0.61027771\n",
      " 0.59490623 0.59411822 0.59411822 0.59411822 0.59451365 0.5956963\n",
      " 0.59556498 0.59490821 0.61080006 0.60869894 0.60961827 0.61001205\n",
      " 0.618816   0.618816   0.618816   0.618816   0.618816   0.618816\n",
      " 0.618816   0.618816   0.618816   0.618816   0.618816   0.618816\n",
      " 0.618816   0.618816   0.618816   0.618816   0.618816   0.618816\n",
      " 0.618816   0.618816   0.618816   0.618816   0.618816   0.618816\n",
      " 0.618816   0.618816   0.618816   0.618816   0.61947277 0.61947277\n",
      " 0.61947277 0.61947277 0.61553318 0.61553318 0.61553318 0.61553318\n",
      " 0.61553318 0.61553318 0.61553318 0.61553318 0.61553318 0.61553318\n",
      " 0.61553318 0.61553318 0.61553318 0.61553318 0.61553318 0.61553318\n",
      " 0.5703402  0.5703402  0.5703402  0.5703402  0.5703402  0.5703402\n",
      " 0.5703402  0.5703402  0.5703402  0.5703402  0.5703402  0.5703402\n",
      " 0.5703402  0.5703402  0.5703402  0.5703402  0.5703402  0.5703402\n",
      " 0.5703402  0.5703402  0.5703402  0.5703402  0.5703402  0.5703402\n",
      " 0.5703402  0.5703402  0.5703402  0.5703402  0.5703402  0.5703402\n",
      " 0.5703402  0.5703402  0.5703402  0.5703402  0.5703402  0.5703402\n",
      " 0.5703402  0.5703402  0.5703402  0.5703402  0.5703402  0.5703402\n",
      " 0.5703402  0.5703402  0.5703402  0.5703402  0.5703402  0.5703402 ]\n",
      "Best cross validation score: 62.35%\n",
      "Optimal parameters: {'classifier__ccp_alpha': 0.0, 'classifier__criterion': 'gini', 'classifier__max_depth': 3, 'classifier__min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "params_dtc1 = {'DecisionTree': [{\n",
    "    'classifier__criterion':['gini', 'entropy'],\n",
    "    'classifier__max_depth':[1, 3, 5, 10, 15, 25],\n",
    "    'classifier__min_samples_split':[2, 5, 6, 8],\n",
    "    'classifier__ccp_alpha':[0.0, 0.01, 0.1]\n",
    "}]}\n",
    "\n",
    "gridsearch_count(params_dtc1, name='DecisionTree', models=models2, num=num_dtc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6d456",
   "metadata": {},
   "source": [
    "Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5d5ebd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Grid Search:\n",
      "RandomForest\n",
      "CountVectorizer(lowercase=False)\n",
      "Scoring method: Accuracy\n",
      "Avg of cross validation scores: [0.58163793 0.57651412 0.57704018 0.57796002 0.57375581 0.58150669\n",
      " 0.57664544 0.57704018 0.57769721 0.57349299 0.580718   0.57664544\n",
      " 0.57664605 0.57769712 0.57533268 0.61001059 0.60764528 0.60567376\n",
      " 0.60567384 0.60725115 0.61250601 0.60738203 0.6058049  0.60606789\n",
      " 0.60764528 0.61040325 0.60764476 0.60843233 0.6067244  0.60895856\n",
      " 0.63299733 0.63115963 0.63431001 0.63181467 0.63299733 0.63509913\n",
      " 0.63221096 0.63352235 0.63142114 0.63168361 0.63536238 0.62892598\n",
      " 0.6314208  0.63260354 0.63247205 0.65637992 0.64836847 0.648367\n",
      " 0.64902533 0.64863137 0.655987   0.64849944 0.65033775 0.65125768\n",
      " 0.65033801 0.65808898 0.64928874 0.65243981 0.6509946  0.64915527\n",
      " 0.5811129  0.57533207 0.5773036  0.57730351 0.57520136 0.58124422\n",
      " 0.57625131 0.57704078 0.57651525 0.57520136 0.57993008 0.5762514\n",
      " 0.57782862 0.5775658  0.57625261 0.61185027 0.60698816 0.60580577\n",
      " 0.60580542 0.60396634 0.61145588 0.60659437 0.60698808 0.60541138\n",
      " 0.60725149 0.61106158 0.60751387 0.60619947 0.60554269 0.60685719\n",
      " 0.63286894 0.63089811 0.63129017 0.6308963  0.63207826 0.63575892\n",
      " 0.63155488 0.63102718 0.63142149 0.63234073 0.63155531 0.63089819\n",
      " 0.63168439 0.63312951 0.63312934 0.65086475 0.64941972 0.64482094\n",
      " 0.64679177 0.64731679 0.65073378 0.64718659 0.64718573 0.64744785\n",
      " 0.6477098  0.64902541 0.64600514 0.64915458 0.64757839 0.64928581]\n",
      "Best cross validation score: 65.81%\n",
      "Optimal parameters: {'classifier__criterion': 'gini', 'classifier__max_depth': 20, 'classifier__min_samples_leaf': 6, 'classifier__n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "params_rf1 = {'RandomForest': [{\n",
    "    \"classifier__n_estimators\": [50,100, 150, 200, 250],\n",
    "    'classifier__criterion':['gini', 'entropy'],\n",
    "    \"classifier__max_depth\": [2, 3, 4, 5, 8, 12, 20],\n",
    "    \"classifier__min_samples_leaf\": [2, 4, 6],\n",
    "    'classifier__max_depth':[5, 10, 15, 20],\n",
    "#    \"classifier__min_weight_fraction_leaf\": [0.1, 0.3]\n",
    "}]}\n",
    "\n",
    "gridsearch_count(params_rf1, name='RandomForest', models=models2, num=num_rfc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c87fc",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d16d3870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Grid Search:\n",
      "MultinomialNB\n",
      "CountVectorizer(lowercase=False)\n",
      "Scoring method: Accuracy\n",
      "Avg of cross validation scores: [0.67358894 0.67477151 0.68015812 0.68554464 0.68856534 0.69382072\n",
      " 0.69487197 0.69789284 0.69999517]\n",
      "Best cross validation score: 70.00%\n",
      "Optimal parameters: {'classifier__alpha': 1}\n"
     ]
    }
   ],
   "source": [
    "params_nb_cv1 = {'MultinomialNB': [{\n",
    "    'classifier__alpha':[.001, .01, .05, .1, .2, .4, .6, .8, 1],\n",
    "\n",
    "}]}\n",
    "\n",
    "gridsearch_count(params_nb_cv1, name=\"MultinomialNB\", models=models2, num=num_mba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e6568",
   "metadata": {},
   "source": [
    "Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "20316f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradBoostClassifier Grid Search:\n",
      "GradBoostClassifier\n",
      "CountVectorizer(lowercase=False)\n",
      "Scoring method: Accuracy\n",
      "Avg of cross validation scores: [0.5703402  0.59542719 0.5703402  0.59333091 0.62603902 0.63943795\n",
      " 0.62380641 0.62525274]\n",
      "Best cross validation score: 63.94%\n",
      "Optimal parameters: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 5, 'classifier__n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "params_gbc1 = {'GradBoostClassifier': [{\n",
    "    'classifier__learning_rate':[.001, .01],\n",
    "    'classifier__n_estimators':[100, 200],\n",
    "    'classifier__max_depth':[5, 10]\n",
    "}]}\n",
    "\n",
    "gridsearch_count(params_gbc1, name='GradBoostClassifier1', models=models2, num=num_gbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ffffb7",
   "metadata": {},
   "source": [
    "Support Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3dbf9cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Grid Search:\n",
      "SVC\n",
      "CountVectorizer(lowercase=False)\n",
      "Scoring method: Accuracy\n",
      "Avg of cross validation scores: [0.65244231]\n",
      "Best cross validation score: 65.24%\n",
      "Optimal parameters: {'classifier__C': 1, 'classifier__gamma': 'scale', 'classifier__kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "params_svc1 = {'SVC': [{\n",
    "    'classifier__C':[1],\n",
    "    'classifier__kernel':['linear'],\n",
    "    'classifier__gamma':['scale'],\n",
    "}]}\n",
    "\n",
    "gridsearch_count(params_svc1, name='SVC1', models=models2, num=num_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41874d42",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gridsearch_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9560/940162493.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m }]}\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mgridsearch_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_svc1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SVC1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodels2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_svc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'gridsearch_count' is not defined"
     ]
    }
   ],
   "source": [
    "params_svc1 = {'SVC': [{\n",
    "    'classifier__C':[1],\n",
    "    'classifier__kernel':['linear'],\n",
    "    'classifier__gamma':['scale'],\n",
    "}]}\n",
    "\n",
    "gridsearch_count(params_svc1, name='SVC1', models=models2, num=num_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "81e6837d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogisticReg': {'classifier__C': 0.1,\n",
       "  'classifier__fit_intercept': True,\n",
       "  'classifier__max_iter': 200,\n",
       "  'classifier__penalty': 'l2',\n",
       "  'classifier__solver': 'saga'},\n",
       " 'DecisionTree': {'classifier__ccp_alpha': 0.0,\n",
       "  'classifier__criterion': 'gini',\n",
       "  'classifier__max_depth': 3,\n",
       "  'classifier__min_samples_split': 2},\n",
       " 'RandomForest': {'classifier__criterion': 'gini',\n",
       "  'classifier__max_depth': 20,\n",
       "  'classifier__min_samples_leaf': 6,\n",
       "  'classifier__n_estimators': 50},\n",
       " 'MultinomialNB': {'classifier__alpha': 1},\n",
       " 'GradBoostClassifier1': {'classifier__learning_rate': 0.01,\n",
       "  'classifier__max_depth': 5,\n",
       "  'classifier__n_estimators': 200},\n",
       " 'SVC1': {'classifier__C': 1,\n",
       "  'classifier__gamma': 'scale',\n",
       "  'classifier__kernel': 'linear'}}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90070bf",
   "metadata": {},
   "source": [
    "These grid searches were significantly lower than the defaults run in the exploratory models notebook so there is something\n",
    "wrong with these grid searches. Random forest is because the max depth was probably significantly higher but the others don't make sense. It is possible this is because there was no shuffle split. It is possible the data would be better if shuffled. This can be done in grid search by train test split."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
